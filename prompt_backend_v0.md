# Backend Agent Instructions for Vibegrapher v0

## Your Mission
Build a FastAPI backend that enables users to "vibecode" OpenAI Agent workflows through natural language with human approval for code changes. No graph visualization in v0.

## Environment Setup

### CRITICAL: Working Directory Convention
**ALWAYS work from the project root directory. NEVER cd into backend/.**
**If a command fails, first check your working directory with `pwd`.**

### Virtual Environment (REQUIRED)
```bash
# STAY IN PROJECT ROOT - do not cd into backend/
# Create and activate virtual environment
python3 -m venv backend/venv
source backend/venv/bin/activate  # Linux/Mac
# or
backend\venv\Scripts\activate  # Windows

# Install dependencies including type checking
pip install -r backend/requirements.txt
pip install mypy sqlalchemy[mypy] types-python-dateutil types-requests

# Configure mypy for type checking
mypy --install-types --non-interactive
```

### Environment Configuration
```bash
# .env (NEVER commit this file - add to .gitignore)
DATABASE_URL=sqlite:///./vibegrapher.db
OPENAI_API_KEY=sk-...
CORS_ORIGINS=*
PORT=8000
```

## Project Organization

### Code Structure
```
# Work from project root, no cd needed - use explicit paths
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ models/         # SQLAlchemy models
â”‚   â”œâ”€â”€ api/            # FastAPI endpoints  
â”‚   â”œâ”€â”€ agents/         
â”‚   â”‚   â””â”€â”€ all_agents.py  # ALL OpenAI agents in ONE file
â”‚   â”œâ”€â”€ services/       # Business logic
â”‚   â””â”€â”€ config.py       # Settings
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ integration/    # PRIMARY: httpx integration tests
â”‚   â””â”€â”€ unit/          # MINIMAL: Only for critical logic
â”œâ”€â”€ alembic/           # Database migrations
â”œâ”€â”€ media/             # Persistent file storage
â”‚   â””â”€â”€ projects/      
â”‚       â”œâ”€â”€ {project.slug}/                    # Git repository for project code
â”‚       â””â”€â”€ {project.slug}_conversations.db   # SQLiteSession persistence for OpenAI agents
â””â”€â”€ validated_test_evidence/  # Test artifacts
```

### Path Convention
**IMPORTANT**: Always work from project root and use explicit paths:
- `backend/app/agents/all_agents.py` not `app/agents/all_agents.py`
- `backend/tests/integration/test_phase_001.py` not `tests/integration/test_phase_001.py`
- `backend/validated_test_evidence/phase-001/` not `validated_test_evidence/phase-001/`
- If you are unsure about a model or API definition, read `spec_datamodel_v0.md`

### Testing Strategy (pytest + httpx)
- **PRIMARY**: Integration tests using httpx AsyncClient
- **Test Database**: Use separate `test_vibegrapher.db` for tests
- **Setup**: Run database reset command before each test suite
- **MINIMAL**: Unit tests only for:
  - Git operations (pygit2)
  - Sandbox isolation
- **FOCUS**: Stateful integration tests that:
  - Create projects
  - Start sessions
  - Send multiple messages
  - Verify conversation context

### Running Tests
```bash
# STAY IN PROJECT ROOT - all paths are relative to root
# Set test database environment variable
export DATABASE_URL=sqlite:///./backend/test_vibegrapher.db

# Reset test database before running tests
python -m backend.app.management.reset_db --test-db

# Run all tests from project root - MUST show logs
pytest backend/tests -s --log-cli-level=INFO

# Run specific phase tests
pytest backend/tests/integration/test_phase001_infrastructure.py -v -s

# Test output should be minimal and factual:
# Running: POST /projects
# Result: 201, project_id=abc123
# Expected: 201

# Run with coverage
pytest backend/tests --cov=backend.app --cov-report=html

# Run tests headless (no UI)
pytest backend/tests --no-header --tb=short
```

## Implementation Phases

**IMPORTANT: Some details in the plan files MAY BE WRONG. They were generated by an LLM. If you find in your own testing they are wrong and that a different approach works better (as long as you meet acceptance criteria), make your UPDATE to the plan file and explain what old information was wrong and what is the replacement.**

See `plans/backend-phase-*.md` for detailed requirements:

1. **Phase 001**: Core Infrastructure â†’ `plans/backend-phase-001-infrastructure.md`
2. **Phase 002**: Socket.io & Real-time â†’ `plans/backend-phase-002-socketio.md`
3. **Phase 003**: Git Service & Database Seeding â†’ `plans/backend-phase-003-git-seeding.md`
4. **Phase 004**: OpenAI Agents â†’ `plans/backend-phase-004-agents.md`
5. **Phase 005**: Session Management â†’ `plans/backend-phase-005-sessions.md`
6. **Phase 006**: Human Review & Diff Testing â†’ `plans/backend-phase-006-human-review.md`
7. **Phase 007**: Production Deployment â†’ `plans/backend-phase-007-deployment.md`

## Critical Requirements

### OpenAI Agents SDK Usage (REAL API ONLY)
- MUST use gpt-5 series models - THESE ARE REAL!
- CRITICAL: NEVER MOCK OpenAI APIs
- ALL tests MUST use real OpenAI API calls with valid API key
- NO mock responses, NO fake agents, NO stubbed models
- This ensures real token usage tracking and authentic responses
- ALWAYS extract and log usage: result.usage and trace_ids.

## Quality Checklist

Before EVERY commit:
- [ ] Working from project root directory (check with `pwd`)
- [ ] Type checking passes (`mypy backend/app/`)
- [ ] Tests pass with logs visible (`pytest backend/tests -s --log-cli-level=INFO`)
- [ ] OpenAI token usage visible in test output
- [ ] No hardcoded URLs/ports
- [ ] No secrets in code (except valid OPENAI_API_KEY in backend/.env)
- [ ] Virtual environment active (backend/venv)
- [ ] All functions have type hints
- [ ] Pydantic models validate all API data
- [ ] SQLiteSession used correctly with file persistence
- [ ] Full OpenAI responses stored with trace_id AND token usage
- [ ] OpenAI calls logged with ðŸ’µ token counts
- [ ] Integration tests cover full flows
- [ ] NO MOCKED OpenAI calls anywhere

## Git Commit Messages
- Keep commits factual and minimal
- Format: "Add/Fix/Update [component]: [what changed]"
- Examples:
  - "Add backend: project CRUD endpoints"
  - "Fix agents: patch validation error handling"
  - "Update tests: add OpenAI token logging"
- No emojis, no business impact statements, no verbose explanations

## Remember
- Focus on integration tests, not unit tests
- FAIL LOUDLY: Log all errors with stack traces
- Always use explicit paths from project root (backend/...)
- If you are unsure about a model or API definition, read `spec_datamodel_v0.md`

# Final Instructions - Infinite Loop Workflow
**Work continuously in this loop until you get stuck with errors:**

1. Read the specs files: `spec_datamodel_v0.md` and `spec_backend_v0.md`
2. Go into the `plans/` directory and find the first backend document that is not done
3. Check first few lines to see if done - do not read whole file
4. Complete that phase entirely
5. Once done, write a header `# DONE as of commit [commit-hash]`
6. **Re-read specs and this prompt file (to refresh context)**
7. **LOOP BACK TO STEP 2** - find the next incomplete backend document
8. **Continue this infinite loop until you get stuck with bugs**
9. **ONLY COMMIT WORKING CODE!** - Stop if code doesn't work

## Deployment Notes (Phase 007)
- Deploy to Fly.io EWR region with PostgreSQL
- Use GitHub Actions for CI/CD (see `plans/backend-phase-007-deployment.md`)
- Production: never scale to 0, autoscale up to 5
- Preview: scale to 0 for PR deployments, use SQLite
- Run migrations on every deployment
- Mount persistent volume for media/projects (production only)